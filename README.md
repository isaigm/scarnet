# Acne Scar Classification: A Rigorous Replication Study

## Abstract

This project began as a replication study of the "ScarNet" paper, which reported 92.5% accuracy in classifying acne scars. During the study, significant methodological challenges were identified, including reproducibility issues and a high risk of **data leakage**, which likely inflated the original results.

In response, a superior and more robust solution was engineered using **Transfer Learning** with a fine-tuned ResNet18. The final strategy involved merging visually similar classes, applying advanced `RandAugment` data augmentation, and implementing a 2-stage progressive fine-tuning process.

To ensure a scientifically sound measure of performance, the final model was subjected to **K-Fold Cross-Validation**. This rigorous validation yielded a final, honest performance metric of **64.4% ± 5.85% average accuracy**, which represents the true expected performance of the model on this challenging, limited dataset.

## Final Model Strategy

- **Core Technique:** ResNet18 with Transfer Learning.
- **Final Performance Metric:** **64.4% ± 5.85%** (5-Fold Cross-Validation Accuracy).
- **Data Strategy:** A 4-class problem, merging the 'Rolling' and 'Boxcar' classes due to their high visual similarity and the data scarcity of the former.
- **Key Methods:**
  - K-Fold Cross-Validation for robust evaluation.
  - 2-Stage Progressive Fine-Tuning.
  - Advanced data augmentation with `RandAugment`.
  - Class imbalance handling with `WeightedRandomSampler`.

## Project Structure

```
/
|-- dataset/
|   |-- Boxcar/
|   |-- Hypertrophic/
|   |-- Ice Pick/
|   |-- Keloid/
|   `-- Rolling/
|-- resnet_4class_randaugment_final_model.pth  <-- A trained model from a single split
|-- train_scarnet.py                           <-- Script to train a single model
|-- cross_validation.py                        <-- Script for rigorous K-Fold Cross-Validation
|-- test_model.py                              <-- Script to evaluate a single model on the full dataset
|-- predict_single_image.py                    <-- Script to predict with a single model
|-- requirements.txt                           <-- Project dependencies
`-- README.md
```

## Installation

It is recommended to use a virtual environment to run this project.

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/isaigm/scarnet
    cd scarnet
    ```

2.  **Create and activate a virtual environment:**
    ```bash
    python -m venv venv
    # On Windows
    .\venv\Scripts\activate
    # On macOS/Linux
    source venv/bin/activate
    ```

3.  **Install the dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## Usage

The recommended workflow is to first verify the robust performance metric using cross-validation, and then train a single model for inference tasks.

### 1. Run K-Fold Cross-Validation (Recommended for Performance Evaluation)

This is the most important script for rigorously evaluating the model's performance. It runs the entire training and testing process 5 times on different data splits and reports the average accuracy and standard deviation, providing the final, unbiased performance metric.

```bash
python cross_validation.py
```
### 2. Train a Single Model

If you want to generate a single `.pth` model file for prediction tasks, use this script. It will run the training process on a single 80/20 split. Note that the final accuracy will vary depending on the random split, as demonstrated by the cross-validation results.

```bash
python train_scarnet.py
```

### 3. Evaluate a Single Trained Model

To analyze the performance of a saved model (like the one generated by `train_scarnet.py`) on all 250 images, run:

```bash
python test_model.py
```
This is useful for generating a global confusion matrix and visualizing specific predictions.

### 4. Predict with a Single Image

To classify a new image with a trained model, use:

```bash
python predict_single_image.py --image "path/to/your/image.jpg"
```
Example:
```bash
python predict_single_image.py --image "dataset/Ice Pick/icepick (1).png"
```

## Performance Evaluation & Key Results

### Rigorous K-Fold Cross-Validation (The Scientific Truth)

To obtain a reliable and unbiased measure of the model's true performance, a 5-Fold Cross-Validation was implemented. This process trains and evaluates 5 separate models on different subsets of the data, eliminating the "lucky split" bias.

-   **Accuracy per Fold:** `[72.0%, 58.0%, 70.0%, 58.0%, 64.0%]`
-   **Average Accuracy:** **64.4%**
-   **Standard Deviation:** **5.9%**

The final, scientifically rigorous result is **64.4% ± 5.9%**. This is the most honest estimate of how the model is expected to perform on new, unseen data. The variance between folds confirms that the dataset is challenging and performance is sensitive to data distribution.

### Single Split Evaluation (The "Lucky Split")

For context, when evaluated on a single, fixed 80/20 train/test split, the model achieved a promising but potentially biased accuracy of **84.4%**. This result highlights the model's potential but underscores the importance of cross-validation for a true measure of performance.

## Key Findings & Conclusion

-   **Irreproducibility of the "ScarNet" Paper:** The original paper's results are likely a product of **data leakage**, a common pitfall that leads to inflated and non-generalizable results.
-   **Superiority of Transfer Learning:** A **Transfer Learning** approach on the standard **RGB** color space was scientifically demonstrated to be a fundamentally superior strategy over training a custom CNN from scratch on this limited dataset.
-   **Importance of Rigorous Validation:** This study underscores that a single train/test split can be misleading. **K-Fold Cross-Validation** provides a much more robust and realistic measure of a model's true capabilities.
-   **Final Verdict:** The developed methodology represents a robust solution to a challenging fine-grained classification problem, with its real-world performance honestly quantified by cross-validation.

## License

This project is distributed under the MIT License.
